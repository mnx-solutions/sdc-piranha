{"cpu":{"generic":"The CPU metrics provide observability into CPU resource usage.  These metrics\nallow operators to understand CPU utilization and saturation and for customers\nto understand their usage of CPU resources and compare that to their limits.","cpu.cpus":"\n\nThis raw metric measures the number of CPUs, which itself may not be very\ninteresting.  However, the raw value can be decomposed by current utilization\nand viewed as a heatmap, allowing operators to quickly see which CPUs are hot\nwithin the datacenter or on a particular server.\n\n\n","cpu.thread_samples":"\n\nThis raw metric counts the number of times a non-idle thread was sampled on-CPU.\nThe sampling is performed at 99 Hertz (samples per second) per CPU, not 100\nHertz, to avoid sampling in lockstep with a timed activity.  This rate means\nthat each sample represents roughly 10 ms of CPU time.  The sampling is\nperformed across all CPUs, so the total samples possible during a second will\nbe 99 x number of CPUs.\n\nThis can be used to understand CPU usage at a coarse-grained level.  The\n\"subsecond\" heatmap shows the time within a second on the y-axis, so that\nregular timed activity can be observed as patterns, indicating how often during\na second a thread was on-CPU doing work.  A timed activity that occured once\nper second at the same time would appear as a single horizontal line.  One that\ncrept over time by performing work and then sleeping for a full second would\nappear as a diagonal line, the slope of which showing how much CPU work was\nperformed during each wakeup: steeper for more.\n\nThis lightweight metric may be a good starting point for CPU usage\ninvestigations.  For more detail, see the CPU thread executions metric.\n\n\n","cpu.thread_executions":"\n\nThis raw metric counts the number of times any thread was taken off CPU (with\nthe exception of the kernel idle theads).  This can be used to understand CPU\nusage at a very fine-grained level, since you can observe which applications\nare running, for how long they're running before being kicked off CPU, and why\nthey're being kicked off CPU.  This in turn can help understand whether an\napplication is actually using a lot of CPU directly (e.g., on CPU for long\nperiods doing computation) vs. not (e.g., on CPU for many short bursts, then\nwaiting for I/O).\n\nThe \"subsecond\" heatmap shows the time offset within a second on the y-axis\nfor when the thread began to execute.  It can be interpreted in the same way\nas the CPU thread samples subsecond heatmap.\n\n\n","cpu.usage":"\n\nThis raw metric reports the percent of CPU time used as a percent of 1 CPU's\nmaximum possible utilization.  For example, if a system has 8 CPUs, the maximum\nvalue for that system will be 800.  On this system, an application fully\nutilizing 2 CPUs for 1 second out of 5 will have a usage of 5% (25% of CPU, 20%\nof the time).  This is most useful for understanding a zone's overall CPU usage\nfor load management purposes.  Also, since CPU caps are defined in terms of\naggregated CPU usage, this metric can show how close a zone is to reaching its\nCPU cap.\n\nIt's important to remember that many applications do not effectively utilize\nmultiple CPUs.  As a result, an application may be compute-bound even though\nits zone is not using all available CPU resources because the application may be\nmaxing out a single CPU.  To investigate this behavior, see the \"CPU: cpus\"\nmetric, which shows the utilization by-cpu, or the \"CPU: thread executions\"\nmetric, which can show the reason why an application is not using more CPU.\n\n\n","cpu.waittime":"\n\nThis raw metric measures the total amount of time spent by runnable threads\nwaiting for a CPU.  The longer the aggregated wait time, the more time threads\nspent waiting for an available CPU while ready to run.  Even on relatively idle\nsystems, it's normal to see non-zero wait time, since there are often more\nthreads ready to run than CPUs.  However, persistent high wait times indicate\nCPU saturation.\n\n\n","cpu.loadavg1":" operators only.\n\nThis raw metric roughly correlates with the average number of threads ready to\nrun at any given time over the last minute.  In raw form or when decomposed by\nhostname, load average reflects the amount of work being done on the system, as\nwell as how much capacity is available for more work.\n\nCare must be taken in interpreting the by-zonename numbers.  Like the\nsystem-wide metric, the load average for a zone reflects the average number of\nthat zone's threads ready to run at any given time over the last minute.\nHowever, a high load average for a zone does not necessarily mean that zone is\ncontributing much load to the system.  For example, a single very active zone on\na system can inflate the load averages of other zones on the system by keeping\nthe CPUs busy and causing other zones' threads to have to wait for the CPU.\nWithin a zone, the load average should be viewed not as a measure of the system\nload induced by the zone but as a measure of the system load that's impacting\nthe zone (which may, of course, be caused by the zone itself).\n\nSee \"CPU: aggregated wait time\" for another measure of CPU saturation.\n\n\n"},"disk":{"generic":"The disk metrics provide observability into disk I/O across a datacenter.","disk.disks":"\n\nThis raw metric measures the number of disks, which itself may not be very\ninteresting.  However, the raw value can be decomposed by percent busy time,\nnumber of I/O operations completed, or number of bytes transferred, and the\nresult viewed as a heatmap.  This allows operators to quickly identify which\ndisks are busy within a datacenter or on a particular server.\n\nSince individual disks have finite limits on both data throughput and IOPS, this\nmetric also allows administrators to identify disks that are maxed out, which\nmay be limiters for application performance.\n\n\n","disk.physio_bytes":"\n\nThis metric measures the raw number of bytes read and/or written to disks.  This\nallows operators to see whether disks are being driven to maximum throughput\n(i.e. whether the workload is disk throughput-bound) as well as the\ndecomposition of read and write operations in the workload.\n\n\n","disk.physio_ops":"\n\nThis raw metric measures the raw number of read and write operations completed\nby disks.  This allows operators to see whether disks are being driven to\nmaximum IOPS throughput (i.e. whether the workload is disk IOPS-bound).\n\nAdditionally, this metric provides decompositions by size and offset, which\nhelp operators understand the nature of the I/O workload being applied, and\na decomposition by latency which provides deep understanding of disk performance\nas it affects the workload.\n\n\n"},"fs":{"generic":"The filesystem metrics provide visibility for logical filesystem operations\nperformed by system software and applications.  This is critically important\nbecause the filesystem is the main interface through which applications access\ndisks, and disks can be a major source of system latency.  However, it's very\nhard to correlate filesystem operations with disk operations for a large number\nof reasons:\n* Filesystem read operations (including \"read\", \"lookup\", etc.) may be satisfied\nfrom the OS cache, in which case the disk may not need to be accessed at all.\n* A single logical filesystem read may require *multiple* disk reads because the\nrequested chunk is larger than disk sector size or the filesystem block size.\n* Even for a single logical filesystem read that's smaller than the disk sector\nsize, the filesystem may require multiple disk reads in order to read the file\nmetadata (e.g., indirect blocks).  Of course, any number of these reads may be\nsatisfied by the OS read cache, reducing the number that actually hit the\ndisk.\n* Writes to files not marked for synchronous access will generally be cached in\nthe OS and written out later.  However, if the write does not change an entire\nfilesystem block, the OS will need to *read* all changed blocks (and the\nassociated file metadata).\n* Even writes that do rewrite an entire filesystem block may require reading\nfile metadata (e.g., indirect blocks).\nIn summary, it's very difficult to predict for a given logical filesystem\noperation what disk operations will correspond to it.  However, it's also not\ngenerally necessary.  To understand application performance, you can use these\nfilesystem metrics to see logical filesystem operation *latency*.  If it's low,\nthen disk effects are not relevant.  Only if filesystem logical operation\nlatency is high should disk performance be suspected.  Similarly, if disk\noperation latency is high, that doesn't mean applications are actually\nexperiencing that latency.","fs.logical_ops":"\n\nThis raw metric measures the total number of logical filesystem operations,\nincluding read, write, create, fsync, ioctl, mkdir, and many others.  The result\ncan be decomposed by host, zone, application, filesystem type, operation type,\nand latency (as a heatmap).  This is a primary metric for understanding\napplication latency resulting from filesystem or disk slowness.  See the\ndescription under \"filesystem-related metrics\" above.\n\n\n","fs.logical_rwops":"\n\nThis raw metric measures the total number of read/write operations.  Unlike the\n\"logical filesystem operations\" metric, this metric *only* counts reads and\nwrites, not the various metadata operations like create, fsync, ioctl, and\nothers.\n\nThe \"size\" heatmap shows the requested size in bytes of each I/O, and the\n\"offset\" heatmap shows the byte location in the file that is being read or\nwritten.  Both of these can be used to characterize the workload applied to\nthe file system by applications, and compared to the resulting workload at\nthe disk level.\n\n\n","fs.logical_rwbytes":"\n\nThis raw metric measures the total number of bytes logically read and written to\nthe filesystem.  This metric *only* counts reads and writes, not the various\nmetadata operations like create, fsync, ioctl, and others.\n\n\n"},"memory":{"generic":"The Memory metrics report physical and virtual memory used by host and zone, as\nwell as events related to memory use like memory reclamations and page-ins.","memory.rss":"\n\nThe resident set of an application is the amount of physical memory it's\ncurrently using.  This metric provides that information in total, by hostname,\nor by zonename.\n\n\n","memory.rss_limit":"\n\nThis metric reports the system-imposed maximum resident set size in total, by\nhostname, or by zonename.  See \"Memory: resident set size.\"\n\n\n","memory.swap":"\n\nThis metric measures the total amount of virtual memory reserved by\napplications, optionally decomposed by hostname and zonename.  The operating\nsystem reserves virtual memory for all memory an application allocates that's\nnot directly backed by the filesystem, including memory allocated with malloc()\n(whether or not the memory has been used) or by privately mapped files.  Each\nzone has a limit on the maximum amount of virtual memory that can be reserved.\nThis metric allows operators and end users to compare zone usage against that\nlimit.\n\n\n","memory.swap_limit":"\n\nThis metric reports the maximum amount of virtual memory reservable by\napplications, optionally decomposed by hostname and zonename.  See \"Memory:\nvirtual memory reserved.\"\n\n\n","memory.reclaimed_bytes":"\n\nThis metric reports the total number of bytes of physical memory (resident set)\nreclaimed by the system because a zone has exceeded its allowable resident set\nsize.  Non-zero values for this metric indicate that a zone is exceeding its\nphysical memory limit and its memory is being paged out.\n\n\n","memory.pageins":"\n\nThis metric reports the total number of pages of virtual memory paged in.\nMemory is paged in when it's needed by an application but is not currently in\nphysical memory because the zone has previously exceeded its physical memory\nlimit.\n\nThis metric is the flip side of excess memory reclaimed: when a zone exceeds\nits physical limit, some memory is paged out, which can be observed with the\n\"Memory: excess memory reclaimed\" metric.  When that memory is needed again,\nit's paged back in, which can be observed using this metric.  In other words,\nthis metric shows when the zone is experiencing latency as a result of having\npreviously exceeded its memory limit.\n\n\n"},"ldapjs":{"generic":"The ldapjs metrics report LDAP protocol activity for Node.js programs using the\nldapjs library version 0.3.2 or later.","ldapjs.connections":"\n\nThis metric reports client connections to an LDAP server, with decompositions\nto identify the server process and client IP address.\n\n\n","ldapjs.ops":"\n\nThis metric reports LDAP operations executed by the server, with decompositions\nto identify the server process, client IP address, and operation details\n(operation type, bind DN, request DN, and status).  The \"latency\" field shows a\nheatmap of operation latency, which is useful for identifying cases where the\nLDAP server is a source of systemic latency.  For such cases, the detail fields\ncan help isolate the types of operations taking the most time.\n\nAdditional information is available for \"search\" operations using the \"ldapjs:\nsearch operations\" metric.\n\n\n","ldapjs.search_ops":"\n\nThis metric reports LDAP \"search\" operations executed by the server, with\ndecompositions to identify the server process, client IP address, and search\nquery details.  This metric is similar to the \"ldapjs: all operations\" metric\nbut isolates only the \"search\" operations and includes search-specific fields\ndescribing the filter and scope.  For investigating poor search performance,\nthese fields can be used to identify which queries are taking the most time.\n\n\n\n"},"mysql":{"generic":"The MySQL metrics report activity for MySQL and Percona master database\nservers.  These are only available for versions which support the DTrace mysql\nprovider, which is currently versions 5.5 and later which have been compiled\nwith ENABLE_DTRACE.  The metrics show connection, command, query, statement\nand filesort details.\nSome of these metrics follow an execution heirarchy.\nThis makes it possible to analyze the MySQL database from different\nperspectives.  The commands metric is the most inclusive, covering queries and\nother command types, and including all latency incurred.  The statements metric\nis most specific, which can show the SQL statement type and result of the\nstatement in terms of rows.","mysql.connections":"\n\nThis shows client connections to the database, providing breakdowns to show\nthe client username and client hostname.  The \"latency\" field shows a heatmap\ndepicting the entire duration of the connection, which may last many seconds\nand span many queries.\n\n\n","mysql.commands":"\n\nThis shows commands that are served by the database, and has a number\nof breakdowns to show general characteristics of the workload: including the\ncommand type, user name and client hostname.  The command result, success or\nfail, can be seen using \"status\".\n\nThe \"latency\" breakdown shows command performance in detail, which can be used\nto identify single outliers and patterns of degraded performance.  The latency\nheatmap can be visually compared to the \"cputime\" heatmap, which shows the time\nspent on-CPU for each command.  If these heatmaps look similar, then the\ncommands are spending most of their time on-CPU in the database.  If the\nlatency heatmap shows much higher latency than the cputime heatmap, then\ncommands are blocked off-CPU for some reason, which can include waiting on file\nsystem I/O (including disk I/O), locks, and for their turn on-CPU.\n\nA query command is one of the command types; others include preparing and\nexecuting statements, and getting statistics.  Because of this, this command\nmetric has a more complete view of database requests than the queries metric.\nThe commands themselves are presented as their numeric values.  The \"Command\nProbes\" section of the MySQL 5.5+ Reference Manual lists commands and their\ndescriptions.  Common command types include:\n\n* 1: close connection\n* 3: execute a query\n* 9: get statistics\n* 22: prepare a statement\n* 23: execute a prepared statement\n\n\n","mysql.queries":"\n\nThis shows the queries performed by the database, including query cache hits\nand the execution of prepared statements.  This metric has a number of\ndecompositions to show general characteristics of the workload: including\nthe database name, user name and client hostname.  The query result, success\nor fail, can be seen using \"status\".\n\nThe \"latency\" and \"cputime\" heatmaps provide detailed performance data, and\ncan be compared in the same way as with the commands metric, described\nearlier.\n\n\n","mysql.statements":"\n\nThis metric shows the execution of SQL statements contained within a query,\nand can be used to characterize the workload applied to the database, and to\ninvestigate performance in terms of latency and number of rows the statements\noperate on.  These are statements that are executed, and do not contain\nqueries that return from the query cache (query cache hits).\n\nThe \"statement\" decomposition will show the statement type, such as \"select\",\n\"insert\", \"update\", \"delete\".  The \"rowsmatched\" decomposition shows the number\nof rows matched by the statement: for SELECT this is the number of rows\nreturned, for INSERT and DELETE the number of rows affected, and for UPDATE\nthis is the rows matched by the WHERE clause - although they may not be\nmodified if their value is already set to the new value.  The \"rowschanged\"\ndecomposition is only valid for UPDATE, and shows the number of rows that were\nactually changed.  The \"latency\" and \"cputime\" heatmaps show the distribution\nof statement execution time, and can be compared in the same way as with the\nqueries metric described earlier.\n\n\n","mysql.filesort":"\n\nThis shows the filesort operation in MySQL databases, which can be expensive\ncomponent of a query, both in terms of CPU time and file system I/O.\nBreakdowns provide information on the database and table that were the subject\nto filesort.  The latency and cputime fields allow the time spent doing\nfilesort, and the time spent on-CPU to be examined.  If these heatmaps are\ndifferent with \"latency\" much higher than \"cputime\", it can indicate that\ntime is spent waiting on file system I/O (including disk I/O) rather than\nperforming work on-CPU.\n\n"},"unix":{"generic":"These metrics report information about the state of processes, when they are\ncreated, and when they exit.","unix.proc_execs":"\n\nThis metric reports each time a process has successfully finished an exec(2) or\nequivalent call. This is useful for understanding how often a new process is\nbeing started. Applications that frequently spawn many short lived processes may\nsee poorer performance. Some applications have options that allow programs to\nspawn new threads instead of processes, which may improve performance.\n\n","unix.proc_exits":"\n\nThis metric reports each time a process finishes. This is useful for\nunderstanding when processes complete and how long they were running. The\nwalltime breakdown provides a heatmap of exits based on process runtime. Having\nmany short lived processes could be symptomatic of performance problems. Having\na daemon (like apache, nginx, sendmail) exit frequently could be symptomatic of\nconfiguration and other errors.\n\n","unix.proc_forks":"\n\nThis metric reports each time a process forks. When a process forks it may\neither be for an exec(2) (see unix.proc_execs) or the child will be used for the\napplication. If an application is seeing high fork rates, there may be\nperformance problems. Note, the \"ppid\" decomposition will be the pid of the\nprocess that called fork(2). \"pexecname\" and \"ppsargs\" are not included because\nthey are identical to \"execname\" and \"psargs\".\n\n","unix.thr_creates":"\n\nThis metric reports each time a thread is created. Several applications create\nthreads on each request. If an application is regularly creating a high number\nof threads per second, that may negatively impact performance.\n\n","unix.processes":"\n\nThis metric reports the total number of processes on the system. This is useful\nfor understanding what is running and characteristics of that set.  Decomposing\nbased upon 'rss' create a heatmap that shows the resident set sizes in use.\nThis can be used to get an idea of the different amounts of memory use by\ndifferent processes. Decomposing by 'execname' is useful for seeing which\napplications are most common. 'nthreads' is a heatmap based upon the number of\nthreads that are in each process. 'pmodel' answers the question of which\nprocesses are 32-bit and which are 64-bit.\n\n"},"nic":{"generic":"The NIC metrics allow operators and end users to observe network activity as it\nrelates to physical network cards (system-wide activity) or VNICs (per-zone\nactivity).","nic.nics":"\n\nThis raw metric measures the number of physical network cards, which itself may\nnot be very interesting.  However, the raw value can be decomposed by the number\nof packets sent and received or the number of bytes sent and received and the\nresult viewed as a heatmap, allowing operators to quickly see which NICs are\nbusy within the datacenter or on a particular server.\n\n\n","nic.bytes":"\n\nThis raw metric measures the number of bytes sent and/or received over physical\nnetwork cards, optionally decomposed by hostname, NIC, or direction.\n\n\n","nic.packets":"\n\nThis raw metric measures the number of packets sent and/or received over\nphysical network cards, optionally decomposed by hostname, NIC, or direction.\n\n\n","nic.vnic_bytes":"\n\nThis raw metric measures the number of bytes sent and/or received by a\nparticular zone's VNICs, optionally decomposed by hostname, zonename, or\ndirection.\n\n\n","nic.vnic_packets":"\n\nThis raw metric measures the number of packets sent and/or received by a\nparticular zone's VNICs, optionally decomposed by hostname, zonename, or\ndirection.\n\n\n"},"node":{"generic":"The Node.js metrics provide high-level visibility into several types of activity\nfor Node programs running v0.4.x or later.  Each metric provides fields for\ndecomposing by host, zone, or application.","node.gc_ops":"\n\nThis metric measures the total number of garbage collection operations for\nNode.js programs, optionally decomposed by type of GC (mark-and-sweep or\nscavenge).  The \"latency\" field enables visualizing GC operation time as a\nheatmap.\n\n\n","node.httpc\\_ops":"\n\nThis metric measures the total number of HTTP client operations for Node.js\nprograms, where each operation consists of a request and a response.  The result\ncan be decomposed by any of several HTTP request properties.  The \"latency\"\nfield enables visualizing HTTP client request latency as a heatmap.\n\n\n","node.httpd\\_ops":"\n\nThis metric measures the total number of HTTP server operations for Node.js\nprograms, where each operation consists of a request and a response.  The result\ncan be decomposed by any of several HTTP request properties.  The \"latency\"\nfield enables visualizing HTTP server request latency as a heatmap.\n\n\n","node.socket_ops":"\n\nThis metric measures the total number of socket read/write operations for\nNode.js programs.  The result can be decomposed by the remote address or port\nand the operation type.  The result can be viewed as a heatmap by operation size\n(how many bytes were read or written) or by how many bytes are buffered inside\nNode.  This last heatmap provides observability into memory usage resulting from\ninadequate flow control.\n\n\n"},"syscall":{"generic":"","syscall.syscalls":"\n\nThis raw metric reports the total number of system calls (syscalls), which\nrepresent application requests to the operating system.  Since applications\ninterface with the filesystem, disks, network, other applications, and the\nsystem itself through syscalls, examining syscalls and syscall latency provides\nlow-level insight into most forms of application latency.\n\nThis metric allows users to examine syscall latency (how long the system call\ntook) using a heatmap decomposed by host, zone, application, or syscall.  The\n\"cputime\" heatmap presents a similar visualization based on the actual CPU time\nused by the syscall rather than elapsed wall clock time.\n\nThe \"subsecond\" heatmap traces the time a syscall was called, showing the time\nwithin a second on the y-axis.  A timed activity that occured once per second\nat the same time would appear as a single horizontal line.  One that crept over\ntime by performing work and then sleeping for a full second would appear as a\ndiagonal line.  These details can help characterize the syscall workload\nperformed by application threads.\n\n\n"},"tcp":{"generic":"The TCP metrics provide visibility into TCP activity and errors.","tcp.accepts":"\n\nThis metric reports the number of TCP connections accepted.  For\nconnection-oriented applications, this metric is useful for understanding new\nclient activity.  Applications seeing many connections from the same remote\nhost might consider using a single persistent connection to avoid the overhead\nof TCP connection setup and teardown.\n\n\n","tcp.connects":"\n\nThis metric reports the number of outbound TCP connections completed.  For\nconnection-oriented applications, this metric is useful for understanding new\nclient activity.  Applications seeing many connections to the same remote\nhost might consider using a single persistent connection to avoid the overhead\nof TCP connection setup and teardown.\n\n\n","tcp.errors":"\n\nThis metric reports the number of TCP errors and can be decomposed by the error\ntype.  Different TCP errors have different underlying causes, all of which can\ncontribute to application latency.  For example, retransmitted segments indicate\npacket loss in the network, which causes application activity to block at least\nas long as the configured TCP retransmit timeout (typically multiple seconds).\n\n\n","tcp.segments":"\n\nThis metric reports the total number of TCP segments (packets) sent and received\nand can be used to observe network activity over TCP.\n\n\n"},"vm":{"generic":"The TCP metrics provide visibility into TCP activity and errors.","vm.exits":"\n\nThis metric measures the number of times a virutal machine must stop running the\nguest and handle some operation. Exits can be caused because of disk and network\nI/O or by activities external to the guest such as receiving an interrupt on the\nsame processor. A large number of exits may be symptomatic of poorer\nperformance.\n\n","vm.irqs":"\n\nThis metric measures the raw number of interrupt requests that are being made to\nthe guest. An interrupt request occurs because of an emulated hardware device\nsuch as a timer, networking interface, or disk drive. This is useful for\nunderstanding how the guest is using its virtualized hardware resources.\n\n","vm.physio_ops":"\n\nThis metric gives visibility into the I/O Virtual Machines are doing to their\nvirtual disks. This is the primary metric for understanding Virtual Machine disk\nperformance.\n\n","vm.physio_bytes":"\n\nThis metric measures the raw number of bytes read and/or written to virtual\ndisks provided to Virtual Machines. This allows operators and end users to see\nif the virtual disks are being driven to maximum throughput.\n\n","vm.thread_samples":"\n\nThis raw metric counts the number of times a vCPU was sampled on-CPU. The\nsampling is performed at 99 Hertz (samples per second) per CPU, not 100 Hertz,\nto avoid sampling in lockstep with timed activity.\n\nThis can be used to understand guest CPU usage at a coarse-grained level. The\nvmmuctx decomposition can be used to see what MMU context is active at the time\nof sampling. The subsecond heatmap provides the same insights as that in CPU:\nthread samples.\n\n\n"},"zfs":{"generic":"The ZFS metrics report on the operation of the ZFS filesystem and how disk\nspace is used by ZFS pools and their filesystems.  Typically, an individual\nserver will have one or more storage pools, each of which may contain any\nnumber of datasets (filesystems and volumes), each of which may contain any\nnumber of snapshots.  Some of these datasets are used by the system itself,\nwhile the others are allocated to individual zones.  Some metrics report by\ndataset or by pool.  Dataset-level metrics provide a \"zdataset\" field for\ndecomposing by dataset name, while pool-level metrics provide a \"zpool\" field\nfor decomposing by pool name.\nZFS filesystems are not fixed in size: by default, storage for each filesystem\nis allocated from a single pool.  Most configurations limit filesystem size by\nspecifying a quota, which can be observed using the metrics below.  ZFS also\nprovides reservations, which guarantee space rather than limit it.\nThe flexibility of ZFS storage configuration makes space accounting complex.  Be\nsure to understand all of the concepts and metrics here before drawing\nconclusions from these metrics.  See the zfs(1M) man page for details.","zfs.arc_ops":"\n\nIn raw form, this metric reports all ARC hits and misses.  These can be\ndecomposed by \"optype\" to identify hits and misses.  Operations can also be\ndecomposed by the usual process fields (pid, application name, arguments, and\nparent process information) to identify processes responsible for ARC\noperations.\n\nWhen filesystem data and metadata is requested, ZFS first looks to satisfy the\nrequest from the in-memory adaptive replacement cache (ARC).  If the request\nhits in the cache, it is satisfied immediately.  If the request misses, ZFS\ngenerally must fetch the data from secondary storage, which often takes\nsignificantly more time.  This metric allows users to identify application\nlatency resulting from reading filesystem data and to measure the overall\nefficacy of the cache for their workload.\n\n\n","zfs.dataset_quota":"\n\nIn raw form, this metric reports the sum of all quotas.  This can be decomposed\nby hostname and ZFS dataset.  This metric only applies to datasets with quotas.\n\nIt's important to note that the sum of all quotas for a single system is not\nrelated to the total storage on that system.  For one, not all filesystems have\nquotas.  Additionally, quotas do not guarantee available space.  Thus, the sum\nof quotas could be less than, equal to, or greater than the total space.\n\n\n","zfs.dataset_unused_quota":"\n\nIn raw form, this metric reports the sum of unused quota for all ZFS datasets.\nThis can be decomposed by hostname and ZFS dataset.  Like the \"quota size\"\nmetric, this metric only applies to datasets with quotas.\n\nThis metric is not quite the same as the difference between \"quota\" and \"used\nspace\".  For one, the \"used space\" metric includes space used by datasets with\nno quota configured, which are not counted here.  Additionally, this metric\nincludes space used by a dataset's children, since that space is counted against\na dataset's quota, while the \"used space\" metric does not include a dataset's\nchildren (since that's reported separately).\n\nIt's also important to remember that since ZFS filesystems allocate from a\ncommon pool of storage, each dataset's unused quota overlaps with that of every\nother dataset (unless reservations are being used).  So it's not necessarily\ntrue that the unused quota is space that's available for use.\n\n\n","zfs.dataset_used":"\n\nIn raw form, this metric reports the sum of used space for all ZFS datasets.\nThis can be decomposed by hostname and ZFS dataset.\n\nThe used space for a dataset includes space used by the dataset itself, its\nsnapshots, and any unused reservation configured on the dataset.  However, this\nmetric does *not* include space used by child datasets, since they're reported\nseparately.\n\nSee the \"ZFS: unused quota\" metric for additional details on free space\naccounting.\n\n\n","zfs.pool_free":"\n\nIn raw form, this metric reports the sum of free space for all ZFS pools.  This\ncan be decomposed by hostname and ZFS dataset.\n\n\n","zfs.pool_used":"\n\nIn raw form, this metric reports the sum of used space for all ZFS pools.  This\ncan be decomposed by hostname and ZFS dataset.\n\n\n","zfs.pool_total":"\n\nIn raw form, this metric reports the sum of all space for all ZFS pools.  This\ncan be decomposed by hostname and ZFS dataset.\n\n"},"ca":{"generic":"These metrics provide visibility into the Cloud Analytics service itself.","ca.instr_ticks":"\n\nThis metric counts the number of ticks by all instrumenters in the CA service.\nEach instrumenter normally ticks once per second to gather data.  The latency\nof each tick measures how long it took the instrumenter to gather data for each\nof its instrumentations.  The subsecond offset measures what time within each\nsecond the tick began.  Together, these heatmaps allow administrators to\nunderstand how much time each instrumenter is spending gathering and reporting\ndata and whether data is being reported on time.\n\n","ca.instr_beops":"\n\nThis metric counts the number of data requests to instrumenter backends.\nDuring each instrumenter tick (see above), the instrumenter requests data from\nsome backend for each active instrumentation.  This metric provides visibility\ninto which instrumenters, backends, instrumentations, and metrics are doing a\nlot of work or taking a long time to gather data.  The latency and subsecond\nheatmaps work similarly to their counterparts for the \"instrumenter ticks\"\nmetric above.\n\nThis metric is essentially a more fine-grained view on instrumenter ticks.\nIt's primarily useful when that metric has already demonstrated that\ninstrumenters are taking a long time to gather data and you now need to\nunderstand which backends are responsible for that time.\n\n","ca.instr_enables":"\n\nThis metric counts the number of times an instrumenter enables a Cloud\nAnalytics instrumentation.  Such events are usually triggered when a user\ncreates an instrumentation for the first time, but they can also occur if an\ninstrumenter restarts, if a new compute node is deployed, or if a customer\nprovisions a new machine.  Using the latency heatmap, operators can monitor the\ntime required to enable an instrumentation on each instrumenter.\n\nThis metric is intended for use in supporting Cloud Analytics.  It is not\nuseful for understanding general system performance.\n\n","ca.instr_disables":"\n\nThis metric counts the number of times an instrumenter disables an\ninstrumentation.  Such events are usually triggered when a user deletes an\ninstrumentation.  Using the latency heatmap, operators can monitor the time\nrequired to disable an instrumentation on each instrumenter.\n\nThis metric is intended for use in supporting Cloud Analytics.  It is not\nuseful for understanding general system performance."}}